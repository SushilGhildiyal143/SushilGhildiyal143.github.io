<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Satya Ram Adda">

  
  
  
    
  
  <meta name="description" content="Breif Overview of Problem Statement: With too much freedom in social media, profanity in comments (i.e. comments that are disrespectful, rude or otherwise likely to make someone leave a discussion) can be a serious issue if not addressed. Moreover, with surge in number users accessing social media these days, this task cannot be done manually with help of moderators. So to automate the identification of toxic comments on social media platforms, The Conversation AI team had come up with an interesting kaggle competition to build a roboust machine learning model that can do this task.">

  
  <link rel="alternate" hreflang="en-us" href="https://asrst.github.io/post/toxic-comments-eda-baselines/">

  


  
  
  
  <meta name="theme-color" content="#4caf50">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:500|B612:400,700|B612+Mono:400,700&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.ecffb345f68e61053f5e26a0da4f6eee.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://asrst.github.io/post/toxic-comments-eda-baselines/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Machines Learn">
  <meta property="og:url" content="https://asrst.github.io/post/toxic-comments-eda-baselines/">
  <meta property="og:title" content="Machine Learning to Identify Toxic Comments - EDA &amp; Baselines | Machines Learn">
  <meta property="og:description" content="Breif Overview of Problem Statement: With too much freedom in social media, profanity in comments (i.e. comments that are disrespectful, rude or otherwise likely to make someone leave a discussion) can be a serious issue if not addressed. Moreover, with surge in number users accessing social media these days, this task cannot be done manually with help of moderators. So to automate the identification of toxic comments on social media platforms, The Conversation AI team had come up with an interesting kaggle competition to build a roboust machine learning model that can do this task."><meta property="og:image" content="https://asrst.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://asrst.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-07T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-09T23:33:22&#43;05:30">
  

  


  





  <title>Machine Learning to Identify Toxic Comments - EDA &amp; Baselines | Machines Learn</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Machines Learn</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Machine Learning to Identify Toxic Comments - EDA &amp; Baselines</h1>

  

  
    



<meta content="2019-09-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2019-09-09 23:33:22 &#43;0530 IST" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Sep 9, 2019</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    28 min read
  </span>
  

  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://asrst.github.io/post/toxic-comments-eda-baselines/&amp;text=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://asrst.github.io/post/toxic-comments-eda-baselines/&amp;t=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines&amp;body=https://asrst.github.io/post/toxic-comments-eda-baselines/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://asrst.github.io/post/toxic-comments-eda-baselines/&amp;title=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines%20https://asrst.github.io/post/toxic-comments-eda-baselines/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://asrst.github.io/post/toxic-comments-eda-baselines/&amp;title=Machine%20Learning%20to%20Identify%20Toxic%20Comments%20-%20EDA%20&amp;amp;%20Baselines" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <h3 id="breif-overview-of-problem-statement">Breif Overview of Problem Statement:</h3>

<p>With too much freedom in social media, profanity in comments (i.e. comments that are disrespectful, rude or otherwise likely to make someone leave a discussion) can be a serious issue if not addressed. Moreover, with surge in number users accessing social media these days, this task cannot be done manually with help of moderators. So to automate the identification of toxic comments on social media platforms, The Conversation AI team had come up with an interesting kaggle competition to build a roboust machine learning model that can do this task.</p>

<p>For this competition, we are provided a annotated dataset of comments from Wikipedia’s talk page edits. Using this as training data, we are challenged to build a multi-headed machine learning model that is capable of detecting different types of of toxicity like obscenity, threats, insults, and identity-based hate better than existing machine learning models (yes, conversational AI team already has a model to do this).</p>

<p>More details of the competition can be found here:</p>

<blockquote>
<p>source: <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">kaggle</a></p>
</blockquote>

<h3 id="data--evaluation-metrics">Data &amp; Evaluation metrics:</h3>

<p><code>train.csv</code> contains a large number of Wikipedia comments (around 100K+) which have been labeled by human raters for toxic behavior. The types of toxicity are:</p>

<p><code>toxic
 severe_toxic
 obscene
 threat
 insult
 identity_hate</code></p>

<ul>
<li>Apart from the above mentioned classes there will be also completely clean comments.</li>
<li>The trained model will be tested on <code>test.csv</code> data using mean column-wise ROC AUC as evaluation metric.</li>
<li>**Mean column-wise ROC AUC = Average of the individual ROC AUC scores of each predicted class. **</li>
</ul>

<p>Submission File Format:</p>

<p>For each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severetoxic, obscene, threat, insult, identityhate). The columns must be in the same order as shown below. The file should contain a header and have the following format:</p>

<p><code>id,toxic,severe_toxic,obscene,threat,insult,identity_hate
 00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5
 0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5
 etc.</code></p>

<h4 id="objective-of-this-notebook">Objective of this Notebook:</h4>

<ul>
<li>Explore the data, perform analysis &amp; build baselines.</li>
</ul>

<h3 id="reading-the-data">Reading the Data</h3>

<pre><code class="language-python">import warnings
warnings.filterwarnings('ignore')
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
data_paths = {}
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        data_paths[filename] = os.path.join(dirname, filename)
        # print(os.path.join(dirname, filename))
        
train_df = pd.read_csv(data_paths['train.csv'])
test_df = pd.read_csv(data_paths['test.csv'])
sub_df = pd.read_csv(data_paths['sample_submission.csv'])
print('Train data shape:', train_df.shape)
print('Columns in Train:', train_df.columns)
</code></pre>

<pre><code>Train data shape: (159571, 8)
Columns in Train: Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',
       'insult', 'identity_hate'],
      dtype='object')
</code></pre>

<p>The provided <code>train.csv</code> is having 159571 rows and 8 columns. Let us now sample to take 5 rows from the data to get visual understanding of what is present.</p>

<pre><code class="language-python">train_df.sample(5, random_state = 1)
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>comment_text</th>
      <th>toxic</th>
      <th>severe_toxic</th>
      <th>obscene</th>
      <th>threat</th>
      <th>insult</th>
      <th>identity_hate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24915</th>
      <td>41e65f43c180838e</td>
      <td>YOU ARE A FAT, GEEKY PRICK WHO HAS NOTHING TO ...</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>75819</th>
      <td>cade9e425d75948e</td>
      <td>Agent X2: Basically thanks - with a 'little' m...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>53891</th>
      <td>8ff87747403ed2e6</td>
      <td>Why are my posts being deleted? \n\nI have tri...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>154159</th>
      <td>a95604c7a938abb3</td>
      <td>"\n\n Controlled Demolitions and Common Sense ...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13040</th>
      <td>2288910537399728</td>
      <td>I do not understand your reply.  //Blaxthos ( ...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<ul>
<li>This is going to be text classification problem...we are given a comment and we need to classify whether it is toxic.</li>
<li>The column <code>comment_text</code> contains the comments text and it will be our main feature.</li>
<li>All the remaining columns except <code>id</code> will be our target variable to predict.</li>
<li>Target varaibles are binary encoded (one hot encoded) &amp; all zeros represents a clean comment.</li>
<li>A single comments can belong to multiple toxicity classes</li>
</ul>

<p>Now lets look into few complete random comments from the data</p>

<pre><code class="language-python">for i in [34, 55345, 124786]:
    display(train_df.loc[i, 'comment_text'])
</code></pre>

<pre><code>'&quot;\nWell, not &quot;&quot;before the process&quot;&quot; but &quot;&quot;before how we do things with subpages&quot;&quot; His RfA is listed on NoSeptember\'s page and you can find it if you look. September 2004 I think. I have my differences with El_C to be sure, but was surprised to see a block, so I left a note. ++: t/c &quot;'



'They are deletionists.'



&quot;Hello \n\nHello, I'm Irene.\n\nI created See Me novel from Nicholas Sparks. I created it because I saw Nicholas Spark's official website about a new novel released coming soon. I ask you, is this page wrong or anything?\n\nThanks.&quot;
</code></pre>

<p>Clearly, comments are largely varying in lengths. Let us see how their distrubution looks like</p>

<pre><code class="language-python">comment_lens = train_df['comment_text'].str.len()
print('Central Tendencies on lengths of comment_text\n', comment_lens.describe())
ax = comment_lens.hist()
</code></pre>

<pre><code>Central Tendencies on lengths of comment_text
 count    159571.000000
mean        394.073221
std         590.720282
min           6.000000
25%          96.000000
50%         205.000000
75%         435.000000
max        5000.000000
Name: comment_text, dtype: float64
</code></pre>

<p><figure><img src="output_8_1.png" alt="png"></figure></p>

<p>From the above outputs we can say these points:</p>

<ul>
<li>Comments have no null values.</li>
<li>Mininum length of a comment is only 6 chars while maximum length can be 5000 characters.</li>
<li>Mean (394 chars) and median (205 chars) are not close indicating the skewness - as a result histogram is left sided</li>
</ul>

<p>Now we will look how many of given comments are clean &amp; how many are toxic (in other words, distrubution of classes to be predicted)</p>

<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

drop_col = ['id', 'is_clean']  # columns not neccessary - can be dropped
text_col = ['comment_text']  # text feature
label_col = [col for col in train_df.columns if col not in text_col + drop_col] # target variables

labels_per_comment = train_df[label_col].sum(axis = 1) # clac no.of labels for each comment

# add a new column to indicate if a comment is toxic (bad) or not (clean).
train_df['is_clean'] = 0
train_df.loc[labels_per_comment == 0, 'is_clean'] = 1
# train_df['is_clean'].value_counts()

print(&quot;Total Clean comments (All 0's in a target row) in train:&quot;,len(train_df[train_df['is_clean'] == 1]))
print(&quot;Total unclean/bad comments (atleast one 1 in a target row)in train:&quot;,len(train_df[train_df['is_clean'] != 1]))
print(&quot;Total label tags (total counts of 1's in target columns):&quot;,train_df[label_col].sum().sum())
</code></pre>

<pre><code>Total Clean comments (All 0's in a target row) in train: 143346
Total unclean/bad comments (atleast one 1 in a target row)in train: 16225
Total label tags (total counts of 1's in target columns): 35098
</code></pre>

<p>Importantly we need to notice that total bad comments &amp; total label tags are not equal, so said intially there are multiple labels for many comments. On Average, there are 2 labels for each comment - but let us see - Now let us see how multiple labels are divided:</p>

<pre><code class="language-python">tags_count = labels_per_comment.value_counts()

# plotting the label counts
plt.figure(figsize=(8,4))
ax = sns.barplot(tags_count.index, tags_count.values, alpha=0.7)
plt.title(&quot;Tags Counts v/s Occurences in Train Data&quot;)
plt.ylabel('# of Occurrences', fontsize=12)
plt.xlabel('# Tag Count', fontsize=12)

#adding the text labels
rects = ax.patches
labels = tags_count.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 10, 
            label, ha='center', va='bottom')

plt.show()
</code></pre>

<p><figure><img src="output_12_0.png" alt="png"></figure></p>

<ul>
<li>maximum no.of tags for a bad comment is 6 (i.e it will belong to all six classes).</li>
<li>This we can be viewed as highericial tagging. (Ex: A comment which is <code>severe_toxic</code> will be also a <code>toxic</code>).</li>
</ul>

<p>Lets visualize how the main six classes are distributed (counts of classes) among the 35098 occurences</p>

<pre><code class="language-python">label_counts = train_df[label_col].sum()

# plotting the label counts
plt.figure(figsize=(8,4))
ax = sns.barplot(label_counts.index, label_counts.values, alpha=0.7)
plt.title(&quot;Counts Per Class&quot;)
plt.ylabel('# of Occurrences', fontsize=12)
plt.xlabel('Label', fontsize=12)

#adding the text labels
rects = ax.patches
labels = label_counts.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 10, 
            label, ha='center', va='bottom')

plt.show()
</code></pre>

<p><figure><img src="output_14_0.png" alt="png"></figure></p>

<p>The above figure tells us that Toxicity type is not evenly spreadout - <code>toxic</code> comments are more common while <code>threat</code> is least occuring class. This is interesting &amp; a bit scary too. In total, there is a high imbalance b/w the clean &amp; unclean comments. Again with in unclean comments, there is imbalance b/w the types of toxicity!</p>

<p>Till now we only analyzed the target varaibles, now we will look at one comment each from the all 6 types of toxic classes.</p>

<pre><code class="language-python">import random
for label in label_col:
    label_df = train_df[train_df[label]==1].reset_index(drop = 1)
    print('\n' + label + ' - comment sample :')
    print(label_df.loc[random.randint(0, len(label_df)-1), 'comment_text'])
    print('\n' + '-'*50)
</code></pre>

<pre><code>toxic - comment sample :
Rjanag, your foul language showed your true white trash mentality. You being white prove more of your bias and hideous intent. Keep your effort in trying to hide the truth. If you can certainly call up your gang to block people from Wikipedia, it only serves as proof of your fear of the truth.

--------------------------------------------------

severe_toxic - comment sample :
what i mean to say is that im a big fucknig idiot and i dont know what im talking about

--------------------------------------------------

obscene - comment sample :
Warning
Get off your high horse with warnings and referring to your talk page as if it were another user. You're no admin, but like the other assclowns that have gotten on my case, you seem to think you've got cred by throwing warnings at me. No one is gonna check the talk page of a low importance-rated article and you know it, you dumbfuck.

--------------------------------------------------

threat - comment sample :
I'll kick the shit out of you ya cunt

--------------------------------------------------

insult - comment sample :
Speculate? A retarded 10 year old child can see what is happening. 122.60.93.162

--------------------------------------------------

identity_hate - comment sample :
Hi, you poor twerps. My money does far more good than your pathetic rule enforcing. Have fun with your fat wives and gay hookers.

--------------------------------------------------
</code></pre>

<ul>
<li>Aah...that's a lot of profanity...!</li>
<li>Comments are having puntuctions &amp; things user want to highlight in captials (or) quotes.</li>
<li>As usually, Contractions are always there. In some cases (very less), there is also text from other languages in b/w english.</li>
<li>comments include lots of chatting/social shortcuts which we generally use on social media.</li>
<li>few comments seems to contain numbers, time and also IP address (might be of user's).</li>
<li>In few cases there are also Device Id's and Urls posted by users.</li>
<li>Some comments are spam - repeating same things 10's of times.</li>
<li>In few rare cases, I found the word <code>unblock</code> before the comment - this might indicate that comment is blocked as it is toxic.</li>
</ul>

<p>Now, let us start exploring into comment texts with some questions which relates to our objective. Our objective here is to classify the bad comments into different to toxicity types.</p>

<ul>
<li>Are Longer comments more toxic ?</li>
<li>did Presence of special characters vary with Toxicity ?</li>
<li>Are spammer more toxic ?</li>
</ul>

<p>These questions will answer how Toxicity of comments vary with different features of comment_text like length, presence of punctuations, capital letters, words, or sentences.</p>

<pre><code class="language-python">import re

#Total chars:
train_df['total_len'] = train_df['comment_text'].apply(len)
test_df['total_len'] = test_df['comment_text'].apply(len)

#Sentence count in comment: '\n' is split &amp; count number of sentences in each comment
train_df['sent_count'] = train_df[&quot;comment_text&quot;].apply(lambda x: len(re.findall(&quot;\n&quot;,str(x)))+1)
test_df['sent_count'] = test_df[&quot;comment_text&quot;].apply(lambda x: len(re.findall(&quot;\n&quot;,str(x)))+1)

#Word count in each comment:
train_df['word_count'] = train_df[&quot;comment_text&quot;].apply(lambda x: len(str(x).split()))
test_df['word_count'] = test_df[&quot;comment_text&quot;].apply(lambda x: len(str(x).split()))


plt.figure(figsize=(18,6))
plt.suptitle(&quot;Are longer comments more toxic?&quot;,fontsize=18)
plt.tight_layout()

# total lengths (characters)
plt.subplot(131)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].total_len, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].total_len, label=&quot;Clean&quot;)
plt.legend()
plt.ylabel('Number of occurances', fontsize=12)
plt.xlabel('# of Chars', fontsize=12)
# plt.title(&quot;# Chars v/s Toxicity&quot;, fontsize=12)

# words
plt.subplot(132)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].word_count, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].word_count, label=&quot;Clean&quot;)
plt.legend()
plt.xlabel('# of Words', fontsize=12)
# plt.title(&quot;# Words v/s comment Toxicity&quot;, fontsize=12)

## sentences
plt.subplot(133)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].sent_count, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].sent_count, label=&quot;Clean&quot;)
plt.legend()
plt.xlabel('# of Sentences', fontsize=12)
# plt.title(&quot;# Sentences v/s comment Toxicity&quot;, fontsize=12)

plt.show()
</code></pre>

<p><figure><img src="output_19_0.png" alt="png"></figure></p>

<ul>
<li>most comments are having less than 25 sentences &amp; less than 250 words</li>
<li>unclean comments are having more no.of words in less no.of sentences.</li>
<li>The distrubution plots of clean &amp; unclean of all three plots are very much overlapping with each others, indicating these features are going to be less significant in differentiating them.</li>
</ul>

<pre><code class="language-python">import string

#Captial letters:
train_df['capitals'] = train_df['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))
test_df['capitals'] = test_df['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))

# #Captials ratio:
# train_df['capitals_percent'] = train_df['capitals']/train_df['total_len'] * 100
# test_df['capitals_percent'] = test_df['capitals']/train_df['total_len'] * 100

# punct count:
train_df['punct_count'] = train_df['comment_text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))
test_df['punct_count'] = test_df['comment_text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))

# smilies:
smilies = (':-)', ':)', ';-)', ';)')
train_df['smilies_count'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(s) for s in smilies))
test_df['smilies_count'] = test_df['comment_text'].apply(lambda comment: sum(comment.count(s) for s in smilies))

#----------plotting------------

plt.figure(figsize=(18,6))
plt.suptitle(&quot;did Presence of special characters vary with Toxicity ?\n&quot;,fontsize=18)
plt.tight_layout()

# words
plt.subplot(131)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].capitals, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].capitals, label=&quot;Clean&quot;)
plt.legend()
plt.ylabel('Number of occurances', fontsize=12)
plt.xlabel('# Capital letters', fontsize=12)
# plt.title(&quot;# Captials v/s Toxicity&quot;, fontsize=12)

# words
plt.subplot(132)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].punct_count, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].punct_count, label=&quot;Clean&quot;)
plt.legend()
plt.xlabel('# of Punctuations', fontsize=12)
# plt.title(&quot;#Punctuations v/s comment Toxicity&quot;, fontsize=12)

## sentences
plt.subplot(133)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].smilies_count, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].smilies_count, label=&quot;Clean&quot;)
plt.legend()
plt.xlabel('# of Smilies', fontsize=12)
# plt.title(&quot;#Smilies v/s comment Toxicity&quot;, fontsize=12)

plt.show()
</code></pre>

<p><figure><img src="output_21_0.png" alt="png"></figure></p>

<ul>
<li>presence of captial letters is more in case of unclean comments, but the distrbutions are overlapping making it a difficult feature for models to extract information.</li>
<li>most of the clean comments are having punctuations less than 100 while for unclean comments it spread to max of 5000 punctuations.</li>
<li>no.of smilies in unclean v/s clean comments is very much similar and unclean comments are having more comments with no.of smilies = 1.</li>
</ul>

<pre><code class="language-python">#Unique word count:
train_df['unique_word_count'] = train_df[&quot;comment_text&quot;].apply(lambda x: len(set(str(x).split())))
test_df['unique_word_count'] = test_df[&quot;comment_text&quot;].apply(lambda x: len(set(str(x).split())))

#Unique ratio:
train_df['unique_word_percent'] = train_df['unique_word_count']/train_df['word_count'] * 100
test_df['unique_word_percent'] = test_df['unique_word_count']/train_df['word_count'] * 100

#----------plotting------------

# comments with unique word count percentage &lt; 25%...they can be spam/referal links/marketing links

plt.figure(figsize=(15,5))
plt.suptitle(&quot;Comments with less-unique-words(spam) are more toxic?&quot;,fontsize = 18)

plt.subplot(121)
plt.title(&quot;% of unique words in comments&quot;)
ax=sns.kdeplot(train_df[train_df.is_clean == 0].unique_word_percent, label=&quot;UnClean&quot;,shade=True,color='r')
ax=sns.kdeplot(train_df[train_df.is_clean == 1].unique_word_percent, label=&quot;Clean&quot;)
plt.legend()
plt.ylabel('Number of occurances', fontsize=12)
plt.xlabel('Percent unique words', fontsize=12)

plt.subplot(122)
sns.violinplot(y = 'unique_word_count',x='is_clean', data = train_df[train_df['unique_word_percent'] &lt; 25], 
               split=True,inner=&quot;quart&quot;)
plt.xlabel('is_Clean', fontsize=12)
plt.ylabel('# of words', fontsize=12)
plt.title(&quot;# unique words v/s Toxicity&quot;)
plt.show()

# train_df[train_df['word_unique_percent'] &lt; 25]
</code></pre>

<p><figure><img src="output_23_0.png" alt="png"></figure></p>

<ul>
<li>There is a wide spread area for unclean points in the unique word percentage range of 1-10%, Interesting there are clean comments as well with lesser number of unique words.</li>
<li>This feature seems carry some significance especially incase of sentences with less unique words.</li>
<li>lets once see how text in clean-spam &amp; unclean-spam comments look like</li>
</ul>

<pre><code class="language-python">## lets have a look how clean &amp; unclean spam comment looks like

print(&quot;Clean Spam example:&quot;)
print(train_df[train_df['unique_word_percent'] &lt; 10][train_df['is_clean'] == 1].comment_text.iloc[3])
print('-'*50)
print(&quot;Toxic Spam example:&quot;)
print(train_df[train_df['unique_word_percent'] &lt; 10][train_df['is_clean'] == 0].comment_text.iloc[25])
</code></pre>

<pre><code>Clean Spam example:
MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD MUD
--------------------------------------------------
Toxic Spam example:
I OWN THIS PAGE YOU FOOLS!! YOU SHOULD FEAR ME!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEORGE BUSH SHOULD GO TO HELL!!!!!!!!! I AM THE GOD OF CHOCOBOS, I WANT MY CHEESE, GEOR
</code></pre>

<ul>
<li><p>we definitely need to penalize the over repeating words (or) there will be chance for models to learn as words like 'George Bush' as indicator as toxicity. so comments with lesser unique words should be preprocessed carefully.</p></li>

<li><p>Tfidf is well known for this penalizing effect.</p></li>

<li><p>TF-IDF Vectorizer</p>

<ul>
<li>TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)</li>
<li>IDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization</li>
</ul></li>
</ul>

<pre><code class="language-python">train_df.to_csv('train_feateng.csv', index = None)
test_df.to_csv('test_feateng.csv', index = None)
</code></pre>

<pre><code class="language-python">def get_topn_tfidf_feat_byClass(X_tfidf, y_train, feature_names, labels, topn):
    
    &quot;&quot;&quot;
    utility function to get top N tfidf features from the tf-idf sparse matrix
    &quot;&quot;&quot;
    
    feat_imp_dfs = {}
    
    for label in labels:
        # get indices of rows where label is true
        label_ids = y_train.index[y_train[label] == 1]
        # get subset of rows
        label_rows = X_tfidf[label_ids].toarray()
        # calc mean feature importance
        feat_imp = label_rows.mean(axis = 0)
        # sort by column dimension and get topn feature indices
        topn_ids = np.argsort(feat_imp)[::-1][:topn]
        # combine tfidf value with feature name
        topn_features = [(feature_names[i], feat_imp[i]) for i in topn_ids]
        # df
        topn_df = pd.DataFrame(topn_features, columns = ['word_feature', 'tfidf_value'])
        # save 
        feat_imp_dfs[label] = topn_df
    return feat_imp_dfs
</code></pre>

<h3 id="transforming-text-data-into-tdidf-vectors--analysing-tfidf-values">Transforming Text Data into TD-IDF vectors &amp; Analysing TF-IDF values</h3>

<h4 id="1-visualising-bigram-features-per-class">1. Visualising Bigram features per Class</h4>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk import pos_tag
from nltk.stem.wordnet import WordNetLemmatizer 
from nltk.tokenize import word_tokenize # Tweet tokenizer does not split at apostophes which is what we want
from nltk.tokenize import TweetTokenizer


tfidf = TfidfVectorizer(ngram_range = (1,1), min_df = 100, 
                        strip_accents='unicode', analyzer='word',
                        use_idf=1,smooth_idf=1,sublinear_tf=1,
                        stop_words = 'english')
X_unigrams = tfidf.fit_transform(train_df['comment_text'])
X_unigrams.shape, len(tfidf.get_feature_names())


feature_names = np.array(tfidf.get_feature_names())
imp_dfs = get_topn_tfidf_feat_byClass(X_unigrams, train_df, feature_names, label_col, topn = 10)

plt.figure(figsize=(15,10))

for i, label in enumerate(label_col):
    plt.subplot(3, 2, i + 1)
    sns.barplot(imp_dfs[label].word_feature[:10], imp_dfs[label].tfidf_value[:10], alpha = 0.8)
    plt.title(&quot;Important UniGrams for the class:{}&quot;.format(label))
    plt.tight_layout()
</code></pre>

<p><figure><img src="output_30_0.png" alt="png"></figure></p>

<ul>
<li>few words like <code>fuck</code> seems to be in every class, but again as this is multi-label classification (multiple tags for each comment) there will be that overlapping.</li>
<li>especially <code>threat</code> class is standing apart with words like <code>kill</code>, <code>die</code>, <code>death</code>.</li>
<li>Interestingly, due to high tf-idf value, word <code>wikipedia</code> has stands in top10 features for the <code>toxic</code> class. which model should not learn.</li>
</ul>

<h4 id="2-visualising-bigram-features-per-class">2. Visualising Bigram features per Class</h4>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(ngram_range = (2,2), min_df = 100, 
                        strip_accents='unicode', analyzer='word',
                        use_idf=1,smooth_idf=1,sublinear_tf=1,
                        stop_words = 'english')
X_bigrams = tfidf.fit_transform(train_df['comment_text'])
X_bigrams.shape, len(tfidf.get_feature_names())

feature_names = np.array(tfidf.get_feature_names())
imp_dfs = get_topn_tfidf_feat_byClass(X_bigrams, train_df, feature_names, label_col, topn = 10)

plt.figure(figsize=(15,12))

for i, label in enumerate(label_col):
    plt.subplot(3, 2, i + 1)
    by_class = sns.barplot(imp_dfs[label].word_feature[:10], imp_dfs[label].tfidf_value[:10], alpha = 0.8)
    plt.title(&quot;Important BiGrams for the class:{}&quot;.format(label))
    for item in by_class.get_xticklabels():
        item.set_rotation(45)
    plt.tight_layout()
</code></pre>

<p><figure><img src="output_33_0.png" alt="png"></figure></p>

<ul>
<li>the top 10 tfidf features in case are not much differentiating, every class are having almost same phrases.</li>
<li>remember, the <code>comment_text</code> is completely unprocessed for now.</li>
<li>There is high chance for Names of persons like famous names <code>george bush</code>, <code>mitt romney</code> (if mentioned repeatedly in the spam) to appear in the bigram features for any class. To avoid this we can preprocess spam comments (comments with &lt; 25% unique words) differently like removing usernames/person names from the comments using regular expressions or POS tags.</li>
</ul>

<h3 id="splitting-the-data-into-training--validation-sets">Splitting the data into Training &amp; Validation sets:</h3>

<p>Before testing the model on the actual test data, we neeed to cross-validate internally that model is trained in a right way. Validation set will also help to tune model properly. I am using sklearn's <code>train_test_split</code> utility function to do this random splitting. I am using <code>random_state = 2019</code>as seed for purpose of reproducability &amp; <code>test_size</code> = 0.2 meaning 20% of data points in the train set will be used as validation data and remaining 80% of data will be used to train the model.</p>

<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(train_df['comment_text'], 
                                                  train_df[label_col], test_size=0.2, random_state=2019)
X_test = test_df['comment_text']
print('Data points in train data after splitting:', len(X_train))
print('Data points in valiadtion data:', len(X_val))
print('Data points in test data:', len(X_test))
</code></pre>

<pre><code>Data points in train data after splitting: 127656
Data points in valiadtion data: 31915
Data points in test data: 153164
</code></pre>

<h3 id="naive-baselines">Naive Baselines:</h3>

<p>Before getting to actual modelling &amp; algorithms. let me build some naive baseline on validation set to get minimal optimal score. These scores will be useful for analysing &amp; selection of the models we will be building.</p>

<h4 id="naive-baseline-1-random-guessing-probabilities-bw-0--1">naive baseline 1: Random guessing probabilities b/w 0 &amp; 1.</h4>

<p>This time instead having common probility...we take random value for probability for every column in row. What good is a machine learing if its results are not even better than a random prediction ?</p>

<pre><code class="language-python">from sklearn.metrics import log_loss, roc_auc_score
y_val_naive1 = np.random.rand(y_val.shape[0], y_val.shape[1])
print('Naive Baseline:', 'Random Guessing')
print('ROC-AUC score :', roc_auc_score(y_val, y_val_naive1))
print('Log Loss:', log_loss(y_val, y_val_naive1))
</code></pre>

<pre><code>Naive Baseline: Random Guessing
ROC-AUC score : 0.5075030929045684
Log Loss: 0.4697888746965487
</code></pre>

<h4 id="naive-baseline-2-predicting-all-probabilities-as-05">Naive baseline 2: Predicting all probabilities as 0.5</h4>

<ul>
<li>Here we are guessing more intelligently...as every comments in validation set as clean. From data analysi we found that train data is highly imbalanced there is high chance that validation set will be also have same distrbutions and predicting every comment as clean will actually give us 90% accuracy...this is also reason why accuracy is not an evalution metric for this problem.</li>
</ul>

<pre><code class="language-python">y_val_naive2 = np.zeros(y_val.shape)
y_val_naive2[:] = 0.5
print('Naive Baseline:', 'Random Guessing')
print('ROC-AUC score :', roc_auc_score(y_val, y_val_naive2))
print('Log Loss:', log_loss(y_val, y_val_naive2))
</code></pre>

<pre><code>Naive Baseline: Random Guessing
ROC-AUC score : 0.5
Log Loss: 0.40652139485133465
</code></pre>

<ul>
<li>So the machine model we are going to build should atleast have validation log loss &lt; 0.40 &amp; roc-auc &gt; 0.5. which seems not that tough to achieve.</li>
<li>Before starting the modelling,I am transforming the train, valid &amp; test data splits into tfidf features with both unigrams &amp; bigrams.</li>
</ul>

<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(train_df['comment_text'], 
                                                  train_df[label_col], test_size=0.2, random_state=2019)
X_test = test_df['comment_text']

tfidf = TfidfVectorizer(ngram_range = (1,2), min_df = 9, strip_accents='unicode', analyzer='word',
                        use_idf=1, smooth_idf=1, sublinear_tf=1,stop_words = 'english')
X_train_tf = tfidf.fit_transform(X_train)
X_val_tf = tfidf.transform(X_val)
X_test_tf = tfidf.transform(X_test)
feature_names = tfidf.get_feature_names()

print('Final Data dimensions after transformations:', X_train_tf.shape, y_train.shape, X_val_tf.shape, y_val.shape)
</code></pre>

<pre><code>Final Data dimensions after transformations: (127656, 52083) (127656, 6) (31915, 52083) (31915, 6)
</code></pre>

<h3 id="modelling-baselines">Modelling: Baselines</h3>

<p>Generally, I will begin with least complex models &amp; will move to complex models based on their time/space complexity.</p>

<h4 id="1-naive-bayes">1. Naive Bayes</h4>

<ul>
<li>Naive Bayes is well know for its Text Sentiment Classification tasks. It is also simple to interpret the model so I am starting with it.</li>
<li>If works well, added advantages includes speed - as worst case complexity is O(Nd) is comparatively low.

<ul>
<li>N = No.of data points</li>
<li>d = dimensions (features)</li>
</ul></li>
</ul>

<pre><code class="language-python">from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import MultinomialNB
model = OneVsRestClassifier(MultinomialNB(), n_jobs = -1)
model.fit(X_train_tf, y_train)
print('model: Naive Bayes')
print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))
y_pred_nb = model.predict_proba(X_val_tf)
print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_nb))
</code></pre>

<pre><code>model: Naive Bayes
mean ROC-AUC on train set: 0.9447035800357547
mean ROC-AUC on validation set: 0.9047281844576962
</code></pre>

<ul>
<li><p>so what is OneVsRestClassifier doing here?</p></li>

<li><p>This is basically wrapper that converts the problem of 6 class multi-label problem into 6 individual binary classifications .i.e it builds 6 multinomialNB models. Let me write this implement what I said &amp; verify my understanding.</p></li>
</ul>

<pre><code class="language-python">model = MultinomialNB()

train_rocs = []
valid_rocs = []

preds_train = np.zeros(y_train.shape)
preds_valid = np.zeros(y_val.shape)
preds_test = np.zeros((len(test_df), len(label_col)))
print('model: Naive Bayes')
for i, label_name in enumerate(label_col):
    print('\nClass:= '+label_name)
    # fit
    model.fit(X_train_tf,y_train[label_name])
    
    # train
    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]
    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])
    print('Train ROC AUC:', train_roc_class)
    train_rocs.append(train_roc_class)

    # valid
    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]
    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])
    print('Valid ROC AUC:', valid_roc_class)
    valid_rocs.append(valid_roc_class)
    
    # test predictions
    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]
    
print('\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))
print('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))
</code></pre>

<pre><code>model: Naive Bayes

Class:= toxic
Train ROC AUC: 0.963033817358675
Valid ROC AUC: 0.9433242109626461

Class:= severe_toxic
Train ROC AUC: 0.960087491016496
Valid ROC AUC: 0.9384428174271573

Class:= obscene
Train ROC AUC: 0.9644609483402
Valid ROC AUC: 0.9402753274549794

Class:= threat
Train ROC AUC: 0.9045076993073891
Valid ROC AUC: 0.8043702907286571

Class:= insult
Train ROC AUC: 0.9607587688582031
Valid ROC AUC: 0.9369958039957598

Class:= identity_hate
Train ROC AUC: 0.9153727553335663
Valid ROC AUC: 0.8649606561769773

mean column-wise ROC AUC on Train data:  0.9447035800357547
mean column-wise ROC AUC on Val data: 0.9047281844576962
</code></pre>

<ul>
<li>And the results of both implementations are excatly same..</li>
<li>Naive Bayes performed pretty well in its zone with 0.9047...</li>
<li>This looks like a good score only until we ecperiment with other models...as they might score a way higher lets see...how linear models like Logistic regression &amp; linear SVM perform with default parameters</li>
<li>From here on I will be using the onevsrest classifier.</li>
</ul>

<h4 id="2-logisitc-regression">2. Logisitc Regression</h4>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
model = OneVsRestClassifier(LogisticRegression(), n_jobs = -1)
model.fit(X_train_tf, y_train)
print('model: Logistic Regression')
print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))
y_pred_log = model.predict_proba(X_val_tf)
print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_log))
</code></pre>

<pre><code>model: Logistic Regression
mean ROC-AUC on train set: 0.9916173033614236
mean ROC-AUC on validation set: 0.9765313638921876
</code></pre>

<h4 id="3-linear-svm">3. Linear SVM</h4>

<ul>
<li><p>Linear SVM will not support probability prediction as hinge loss is unstable, the way around is using a calibration classifer wrapper and it wont support OnevsRest classifers. so we I am using manual python loop again.</p></li>

<li><p>Notice that I fitted a new <code>CalibratedClassifierCV</code> wrapper afer fitting the <code>LinearSVC</code> classifier.</p></li>
</ul>

<pre><code class="language-python">from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV

model = LinearSVC()

train_rocs = []
valid_rocs = []

preds_train = np.zeros(y_train.shape)
preds_valid = np.zeros(y_val.shape)
preds_test = np.zeros((len(test_df), len(label_col)))
print('model: Linear SVM')
for i, label_name in enumerate(label_col):
    print('\nClass:= '+label_name)
    
    # fit
    model.fit(X_train_tf,y_train[label_name])
    
    # calibration classifier fit
    model = CalibratedClassifierCV(model, cv = 'prefit')
    model.fit(X_train_tf, y_train[label_name])
    
    # train
    preds_train[:,i] = model.predict_proba(X_train_tf)[:,1]
    train_roc_class = roc_auc_score(y_train[label_name],preds_train[:,i])
    print('Train ROC AUC:', train_roc_class)
    train_rocs.append(train_roc_class)

    # valid
    preds_valid[:,i] = model.predict_proba(X_val_tf)[:,1]
    valid_roc_class = roc_auc_score(y_val[label_name],preds_valid[:,i])
    print('Valid ROC AUC:', valid_roc_class)
    valid_rocs.append(valid_roc_class)
    
    # test predictions
    preds_test[:,i] = model.predict_proba(X_test_tf)[:,1]
    
print('\nmean column-wise ROC AUC on Train data: ', np.mean(train_rocs))
print('mean column-wise ROC AUC on Val data:', np.mean(valid_rocs))
</code></pre>

<pre><code>model: Linear SVM

Class:= toxic
Train ROC AUC: 0.9978703697448817
Valid ROC AUC: 0.96089758073119

Class:= severe_toxic
Train ROC AUC: 0.9866976038380895
Valid ROC AUC: 0.9840277745892984

Class:= obscene
Train ROC AUC: 0.9871142993892349
Valid ROC AUC: 0.9766191922810586

Class:= threat
Train ROC AUC: 0.957937067118062
Valid ROC AUC: 0.9371750021897756

Class:= insult
Train ROC AUC: 0.9816784291545445
Valid ROC AUC: 0.9723417577232863

Class:= identity_hate
Train ROC AUC: 0.9618547477210576
Valid ROC AUC: 0.9461706736620115

mean column-wise ROC AUC on Train data:  0.978858752827645
mean column-wise ROC AUC on Val data: 0.9628719968627699
</code></pre>

<h4 id="performance-of-linear-models">Performance of Linear Models:</h4>

<ul>
<li>Both models Logistic &amp; SVM are performing better than naive bayes models.</li>
<li>Although Logistic is better with default parameters, I need to experiment &amp; see if fine tuning helps Linear SVM.</li>
<li>Moreover, these models are interpretable &amp; not very complex in terms of time &amp; space required.</li>
<li>Further with prerocessing &amp; fine tuning of models I am hoping to reach ROC-AUC of 0.98.</li>
</ul>

<p>Now I will be moving to baselines of non-linear decision tree models namely RandomForests, XGB and Lightgbm.</p>

<h4 id="4-randomforests">4. RandomForests:</h4>

<p>why not single decision tree:</p>

<ul>
<li><p>In my practice I found RandomForest always outperforms decision trees...in other words where ever decision trees work well..certainly randomforests will also do as its base learner is a decision tree.</p></li>

<li><p>Randomforests also outperforms as it averages the decision from 100's of decision tree models. Moreover, decision tree easily tends to overfit relying only on a single decision tree.</p></li>
</ul>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
model = OneVsRestClassifier(RandomForestClassifier(), n_jobs = -1)
print('model: Random Forest')
model.fit(X_train_tf, y_train)
print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))
y_pred_rf = model.predict_proba(X_val_tf)
print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_rf))
</code></pre>

<pre><code>model: Random Forest
mean ROC-AUC on train set: 0.9998282082745614
mean ROC-AUC on validation set: 0.8719848045825899
</code></pre>

<h4 id="5-lightgbm">5. LightGBM</h4>

<ul>
<li>optimized version and generally takes lesser time than XGBoost.</li>
</ul>

<pre><code class="language-python">from lightgbm import LGBMClassifier
model = OneVsRestClassifier(LGBMClassifier(), n_jobs = -1)
print('model: Lightgbm')
model.fit(X_train_tf, y_train)
print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))
y_pred_log = model.predict_proba(X_val_tf)
print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_log))
</code></pre>

<pre><code>model: Lightgbm
mean ROC-AUC on train set: 0.966037582982347
mean ROC-AUC on validation set: 0.9240550756463076
</code></pre>

<h4 id="6xgboost">6.XGBoost</h4>

<pre><code class="language-python">from xgboost import XGBClassifier
model = OneVsRestClassifier(XGBClassifier(), n_jobs = -1)
print('model: XGBoost')
model.fit(X_train_tf, y_train)
print('mean ROC-AUC on train set:', roc_auc_score(y_train, model.predict_proba(X_train_tf)))
y_pred_lgb = model.predict_proba(X_val_tf)
print('mean ROC-AUC on validation set:', roc_auc_score(y_val, y_pred_lgb))
</code></pre>

<pre><code>model: XGBoost
mean ROC-AUC on train set: 0.9498677089044373
mean ROC-AUC on validation set: 0.9366437176804424
</code></pre>

<ul>
<li>Random Forest are serverly overfitting with train ROC-AUC at 0.99 and validation at 0.87</li>
<li>All these non-linear models might be failing case of classes like 'threat' with only around 500 samples for this class which could be a reasoning - bagging &amp; boosting requires more data ?</li>
<li>Non-linear models in this case are overfitting a bit, when compared to linear models.</li>
<li>Guessing, Non linear models can be improved by reducing features using selecting only Top K best or by limiting max_depth, increasing num_trees and tuning other hyperparameters . But ensmebles are also more time complex so sticking with simple linear models is better in this case.</li>
</ul>

<h3 id="conclusions">Conclusions:</h3>

<ul>
<li>Linear models are very well suited for this problem</li>
<li>Logistic Regression is having better baseline score than any other model</li>
<li>Logistic also has an edge with lower time/space complexity compared to ensembles and also, we can impove this further by text preprocessing &amp; hyper parameter tuning.</li>
<li>So rather than trying complex models, I will settle with linear models &amp; will try to improve the performance using preprocessing, finetuning &amp; feature engineering. My next target is to improve to score to more than 0.98 using only linear models.</li>
</ul>

<h3 id="how-about-using-deep-neural-nets-cnns--lstms-">how about using deep neural nets, CNNs &amp; LSTMs ?</h3>

<ul>
<li>with amount of data we are having it is possible for deep neural nets might provide better results.</li>
<li>But Let say after preprocessig &amp; tuning our final Logistic model is having ROC-AUC of 0.98 on average &amp; for some LSTM model is having ROC-AUC of 0.99..I am question myself, is it worth training &amp; deploying such complex model which we can't interpret for an improvement of just 0.01 score ? If would <strong>NO</strong> but in the kaggle competitions, every improvement in 0.0001 score matters...so may be we can try LSTMs after all to improve our knowledge..but in production I would prefer to say NO Again it ultimately depends on serverity of the problem &amp; several other factors.</li>
</ul>

<pre><code class="language-python">X_train_val = train_df['comment_text']
y_train_val = train_df[label_col]

X_train_val = tfidf.fit_transform(train_df['comment_text'])
X_test = tfidf.transform(test_df['comment_text'])


model = OneVsRestClassifier(LogisticRegression(), n_jobs = -1)
model.fit(X_train_val, y_train_val)
print('model: Logistic Regression')
print('mean ROC-AUC on train set:', roc_auc_score(y_train_val, model.predict_proba(X_train_val)))
y_test_pred = model.predict_proba(X_test)
</code></pre>

<pre><code>model: Logistic Regression
mean ROC-AUC on train set: 0.991368050986793
</code></pre>

<pre><code class="language-python">## making a submission file
sub_df.iloc[:,1:] = y_test_pred
sub_df.head()
from IPython.display import FileLink
sub_df.to_csv('submission.csv', index = None)
FileLink('submission.csv')
</code></pre>

<p><a href='submission.csv' target='_blank'>submission.csv</a><br></p>

    </div>

    


    



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://asrst.github.io/">Satya Ram Adda</a></h5>
      <h6 class="card-subtitle">Data Analyst</h6>
      <p class="card-text" itemprop="description">My research interests include distributed robotics, mobile computing and programmable matter.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
          <li>
            <a itemprop="sameAs" href="mailto:addasaiteja@gmail.com" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/" >
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/Asrst" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/writing-technical-content/" rel="prev">Writing technical content in Academic</a>
  </div>
  
</div>

    </div>
    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
